{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![flexsys_logo.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFoAAABFCAYAAADKKPFMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAGOAAABjgBco5mFwAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAUySURBVHic7ZxNiBxFGIbfr2dMgpOkN+q6xhBD1MN2dA2J6MGLXgRPXtRcRcL6gyBGFA9hd34i6EEQBUFQEAQPUdCIgkJWDx4EUdlFyQwxEhHMZvPLzmZ3NrLpej2soJmuqrF7Zmp7l3qO885X9fXbM1Vf/1QJPFdRC3ftI3hYpwkwMd5sPJil3aC7tNYeBMWsgVnb9UY7whvtCG+0I7zRjihmDaxt3vUZhdtShAwAYpxoAEAKeHj8Yv1Y1pzyTGajKWoEkJ0po6yqirE+az55xw8djvBGO8Ib7QhvtCNyZXQRKlf59JLMVQfsJcQJAJcBKbULArWDkIIuKA4Kg13kk4CAVAFtSVkBVC/76kQ3Rttq4mfLzcZRnVANo9MAbtI2SMx0kU+CWhj9IMDdWk1k7/hsfbKX/dlYs3/VvOGNdoQ32hHeaEd4ox3RTdXRc0h8WA2jVqoYYLLSbIxqRZG3hdiqkwpLwXSGFO+qhtFHOmFT89LjL+DPRVNgrowGGKWNEGDBpJVn6+93l0+CIQCP6YSlG0r7cd4cuBaGjjtXOoH/w6o3WgDtVWbeWPVGrxa80Y7I2WTYW2pbhkegsE6nqVKpUZn+KVWF0w25MpqQZwD+ppEeEOCgPgYnje0p+RzADq3YurwHwFTKFH8G5BWdsHh+6yJw3BiYK6MDwffjs43EHbVqGF1vCbvUx5TaOVNu1j/WS3VroB+jHeGNdoQ32hHeaEd4ox3hjXaEN9oR3mhHeKMd4Y12hDfaEd5oR3ijHeGNdoQ32hHeaEd4ox3hjXbESjzKMr7AriCGVQTqFtNvQoCd1TB6TacR2GLqTMjnqmF0NhFDRmJadypIs4D1KpwbXW42tG/7W2GwzXR6CGwH8LJOsy7TBZ/QxtgW9xJD1iYtODf69VvvmyisX3dj++dq6cpS69y5R8eax393nZMLnBtdGhq8/envjiReAfji+crZyQ8Ob3SdTxrYxRN3PxmmIAB+7SI2H0iHnQ9WO7kxGgIoy35Gq51cvalkIpBr3iT/+lGnSSAtEn+4yEMoc1lji7VwWLvgEQBUXFiszB+zv+vkglhtYCHQ17DEhfFm4xOddOi64VEV6xZ0igqIN8bmGifalUoY7RXwSW1fBZnC8qrgBLWBO3aT6iHDEagiIV/DcKEgBV4EcJshuKeICAIWtUMHg3g3IIMEv9LIYwDe08WpGKOA3KORWkrwKTSmFYUjMeUpbR6KUwDe0WqM74VIicS37ZoArxZFsEDyZl0wgMSV00ohgnp5tjHR/nktjF5K3xji7DvYWaD8UmnWtTm6nwzJ1IdI6a0tthlXKdNtgE5tmuMISH6qjr5gO3h9hWPfrTF7VVSkgu0UmxXKEQbQb/twBadTZyICZemP7F3pZ9sW00anIDF8RQBm3yUs4FugYSOTzZtmMJ+1ZTcIO/qWgJ29NpLZaKEcJYrfJD4HRzC/eAjAl+kaNB+EQMi+zF6964sdxhWr0bZAAU6Wm/VEGVQdiA5k8UREGFhyNf0tO2DIRMzSskr9r7fDjVdL/j2fDKWLrX9dQhSynLjMQ0dfqg7rHszm6q4v+zanJWtfIvaysOdGZzZFBMpSWWSsFEy5WNvKFAT7BNuXX7RtPLU+KnKIZDhx3VUdATcK5ZShYeOJIDBcC6PETmAktpN80RQ3Nz2z4d37HznDWMX8zzgyd2qmZJoMCaFADtTCaF97c5Tklm//Igsgp6X9ypK41nRLVqAIyCyA1vJXsQTwn63b5IKpJwKE4GAtjPZrtD1/Ay3aiOaEeIwPAAAAAElFTkSuQmCC) Department of Artificial Intelligence and Systems Engineering, Â©2024, BME-MIT, Mihaly Vetro, Dr. Gabor Hullam  \n",
        "# **VIMIAC16 - Artificial Intelligence**\n",
        "## 2024 Fall Semester\n",
        "## **LAB 7**"
      ],
      "metadata": {
        "id": "8iKVb83HKRvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble learning**"
      ],
      "metadata": {
        "id": "Gy0skNGnD4UU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rfpIun-GBD-J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Motivation**\n",
        "\n",
        "When solving a supervised learning task using machine learning models, we usually utilize only *one predictive model*, which - depending on the nature of the problem (e.g. what do we want to predict) and practical aspects (e.g. how much data we have and how noisy it is) - can be a decision tree, logistic regression model, neural network, etc...\n",
        "\n",
        "The question may arise, however, whether it is possible to improve the accuracy and reliability of the forecast by using the ensemble (e.g. average or other aggregated version) of the predictions of *multiple different models* instead of a single model. The process of building several different models to solve the same (or very similar) problem and then aggregating the predictions of these models to produce the final output is called **ensemble learning**.\n",
        "\n",
        "The drawbacks of ensemble learning are easy to see, mainly because instead of a single model, we have to create a set of models, which requires more resources. In other aspects, however, the aggregate predictive performance of the set of models resulting from ensemble learning usually outperforms both the individual performance and the generalisation ability of the models that make it up.\n",
        "\n",
        "> The latter fact is easy to see through a real-life example: as several empirical results have shown, some people's estimation ability is usually inaccurate due to individual bias. This becomes obvious if, for example, we ask someone to estimate by sight how many marbles are in a transparent jar, where most of the estimates will be significantly different from the actual number. However, if a sufficiently large number of people's estimates are averaged out, the result will be very close to the real number. The main reason for this is that the error of each estimate is (presumably) independent of the others, and this kind of error can be reduced by aggregating the results of several experiments.\n"
      ],
      "metadata": {
        "id": "vN6xPLAHD8yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Bagging and random forests**\n",
        "\n",
        "One of the most fundamental ways to achieve ensemble learning is **Bagging**, which, in a slightly simplified way, consists of the following steps:\n",
        "\n",
        "1. Consider a dataset consisting of $N$ records and $K$ variables.\n",
        "2. Select a real subset of variables $k \\in K$ (these are the columns of the data matrix of size $K*N$), and then perform **Bootstrapping** i.e. sample with replacement a set of records of size $N$ over the selected variables.\n",
        "3. Fit a model to the resulting dataset.\n",
        "4. Repeat steps **2** and **3** until a predefined number of models is obtained.\n",
        "5. (**Aggregation**) The prediction of the resulting model set on the new sample is determined by majority voting in case of classification and by averaging in case of regression.\n",
        "\n",
        "> Note that the dataset sampled in step **2** (**Bootstrapping**) contains only a subset of the variables and samples, and a sample may be included more than once due to replacement sampling. This step is particularly important to ensure that even for models with a deterministic (e.g. decision tree) or a low-variance (e.g. logistic regression) learning process, the procedure creates a heterogeneous model set, as each model learns on different subsets of the original dataset.\n",
        "\n",
        ">Note: the original bagging algorithm always performed bootstrap sampling on the entire variable set. For random forests, however, it is preferable to have the set of variables also randomly sampled, as then the individual models are less correlated with each other. The general name for this technique is *Feature Bagging*, for random forests it is part of Bagging.\n",
        "\n",
        "The schematic flowchart of the procedure in a simplified case creating 3 models is as follows:\n",
        "\n",
        "<!-- ![bagging_algorithm](https://share.mit.bme.hu/index.php/s/RenmdjWN8LtXNfA/download/bagging_algorithm.png) -->\n",
        "\n",
        "<img src=\"https://share.mit.bme.hu/index.php/s/RenmdjWN8LtXNfA/download/bagging_algorithm.png\" alt=\"drawing\" width=\"900\"/>\n",
        "\n",
        "A possible implementation of the Bagging algorithm used for classification is shown below.\n",
        "\n",
        "**Examine the code, then run the code block to define the class!**\n"
      ],
      "metadata": {
        "id": "swI0MARWXrGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, clone\n",
        "from sklearn.utils import resample\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import mode\n",
        "\n",
        "class BaggingClassifier:\n",
        "    \"\"\"\n",
        "    An implementation of the Bagging algorithm for classification tasks\n",
        "    for an arbitrary base estimator.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_estimator : object\n",
        "        A base estimator object implementing the `fit` and `predict` methods.\n",
        "        If None, then the base estimator is a decision tree.\n",
        "        Default: DecisionTreeClassifier()\n",
        "\n",
        "    n_estimators : int\n",
        "        The number of base estimators in the ensemble.\n",
        "        Default: 100\n",
        "\n",
        "    max_features : int or Nonep\n",
        "        The number of features to consider when looking for the best split.\n",
        "        If None, all features are considered.\n",
        "        Default: None\n",
        "\n",
        "    random_state : int or None\n",
        "        The seed for the random number generator.\n",
        "        If None, the random number generator is not seeded.\n",
        "        Default: None\n",
        "    \"\"\"\n",
        "    def __init__(self, base_estimator=DecisionTreeClassifier(), n_estimators=100, max_features=None, random_state=None):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.estimators_ = []\n",
        "        self.features_ = []\n",
        "\n",
        "    @staticmethod\n",
        "    def bootstrap_samples_(X, y, max_features=None, random_state=None):\n",
        "        \"\"\"\n",
        "        Bootstrap samples from the input data.\n",
        "        Samples are drawn with replacement.\n",
        "        If max_features is specified, only a random subset of features is considered.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target values.\n",
        "\n",
        "        max_features : int or None\n",
        "            The number of features to consider when looking for the best split.\n",
        "            If None, all features are considered.\n",
        "            Default: None\n",
        "\n",
        "        random_state : int or None\n",
        "            The seed for the random number generator.\n",
        "            If None, the random number generator is not seeded.\n",
        "            Default: None\n",
        "        \"\"\"\n",
        "        # Sample the dataset with replacement sampling\n",
        "        X_sampled, y_sampled = resample(X, y, replace=True, random_state=random_state)\n",
        "        # Subsample the features if the max number of features is specified\n",
        "        if max_features is not None and max_features < X.shape[1]:\n",
        "            if random_state is not None:\n",
        "                np.random.seed(random_state)\n",
        "            features = np.random.choice(X.shape[1], size=max_features, replace=False)\n",
        "            X_sampled = X_sampled[:, features]\n",
        "            # Return the feature indices along the subsampled data\n",
        "            return X_sampled, y_sampled, features\n",
        "        return X_sampled, y_sampled\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model to the input data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target values.\n",
        "        \"\"\"\n",
        "        self.estimators_, self.features_ = [], []\n",
        "\n",
        "        # Produce the specified number of estimators\n",
        "        for i in range(self.n_estimators):\n",
        "            # Initialize a local version of the random state, if specified.\n",
        "            # Otherwise the bootstrapped samples would be the same for every model when the random state is specified.\n",
        "            if self.random_state is not None:\n",
        "                local_random_state = self.random_state * (i + 1) * self.n_estimators\n",
        "            else:\n",
        "                local_random_state = None\n",
        "\n",
        "            if self.max_features is not None and self.max_features < X.shape[1]:\n",
        "                # Sample the dataset with replacement and the specified number of features\n",
        "                X_sampled, y_sampled, features = self.bootstrap_samples_(X, y, max_features=self.max_features,\n",
        "                                                                         random_state=local_random_state)\n",
        "                self.features_.append(features)\n",
        "            else:\n",
        "                # Sample the dataset with replacement if the number of features is not specified\n",
        "                X_sampled, y_sampled = self.bootstrap_samples_(X, y, random_state=local_random_state)\n",
        "\n",
        "            # Train a copy of the base estimator on the sampled data\n",
        "            estimator = clone(self.base_estimator).fit(X_sampled, y_sampled)\n",
        "            self.estimators_.append(estimator)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the input data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input data.\n",
        "        \"\"\"\n",
        "        if self.max_features is not None and self.max_features < X.shape[1]:\n",
        "            # Predict on the feature subset used for each estimator\n",
        "            predictions = np.array([estimator.predict(X[:, features]) for estimator, features in zip(self.estimators_, self.features_)])\n",
        "        else:\n",
        "            # Predict on the entire feature set for each estimator\n",
        "            predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n",
        "        # Return the average predicted value for every input sample\n",
        "        return mode(predictions, axis=0)[0]"
      ],
      "metadata": {
        "id": "A8A9722-XAGj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above class, the bootstrapping step is handled by a separate static function (`bootstrap_samples_`), and the aggregation is performed by the `predict` function by returning the mode of each model's prediction (`mode` function), which is equivalent to a majority vote.\n",
        "\n",
        "To test the method, we then create an artificially generated dataset and split it into a training and test set using the code block below:"
      ],
      "metadata": {
        "id": "Eh6fnVJ51xh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a dummy classification problem\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=16, n_redundant=0,\n",
        "                           flip_y=0.01, n_clusters_per_class=2,\n",
        "                           hypercube=True, class_sep=1.,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kgF_BNbLStOI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting dataset consists of randomly generated samples that form a given number of normally distributed clusters in the n-dimensional space for each class. For more information, see the documentation of the [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function of the `sklearn` package.\n",
        "\n",
        "With the above settings, a dataset consisting of a total of 1000 samples and 20 continuous variables is produced, which is divided into a training subset of 800 samples and a test subset of 200 samples. Their shape can be checked as follows:"
      ],
      "metadata": {
        "id": "7bMQKQfe4hMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "g0qkl38ghnCN",
        "outputId": "27276694-2a9b-400e-e08b-072ca8970295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800, 20), (200, 20), (800,), (200,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define the following function, which performs the training of a predefined predictor on the generated dataset and then returns the accuracy of the trained model on the test set:"
      ],
      "metadata": {
        "id": "cHvM8emt8_3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_classifier(classifier, runs=10):\n",
        "    accuracies_local = []\n",
        "    for _ in range(runs):\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracies_local.append(accuracy)\n",
        "    return np.mean(accuracies_local)"
      ],
      "metadata": {
        "id": "Hg-4gpUd7xoB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the learning process is non-deterministic (due to bootstrapping), the evaluation function above repeats the process several times and returns the average of the accuracies, so the resulting uncertainty of the accuracy value is lower.\n",
        "\n",
        "Finally, we initialize the bagging classifier as below and observe how it performs in terms of accuracy on the artificial dataset:"
      ],
      "metadata": {
        "id": "x4m3gZNt9kif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create a BaggingClassifier with Decision Trees as base estimators\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                  n_estimators=20,\n",
        "                                  max_features=int(np.sqrt(X.shape[1])))\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "l5xcnI9CSHSV",
        "outputId": "8bc06e9a-ea79-4a3c-d87e-bf16e057b736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy (Decision Tree): 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, a decision tree with a limited maximum depth (`base_estimator=DecisionTreeClassifier(max_depth=3)`) was given as the base model, from which 20 instances (`n_estimators=20`) were created to form a model ensemble. In addition, we specified that the bootstrapped samples for each model should contain a number of variables equal to the square root of the size of the total variable set (rounded to integer: `max_features=int(np.sqrt(X.shape[1]))`). The choice of the base model here is not accidental, as the widely known [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) algorithm is also based on the bagging method with a decision tree base model. However, it is important that the parameters of the bagging algorithm (including the base model) can be chosen arbitrarily.\n",
        "\n",
        "**In the code block below, define and evaluate a bagging classifier similar to the one above, with different parameters!**\n",
        "\n",
        "1. **Note the effect of the depth of the decision tree and the number of models on accuracy!**\n",
        "2. **Try a different classifier as a base model!**\n",
        "\n",
        "> Tip: for the base model, you should choose one of the following classifiers defined in the `sklearn` package:\n",
        "1. [Support-Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (RBF kernel is recommended)\n",
        "2. [Naive Bayes](https://scikit-learn.org/dev//modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
        "3. [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "4. [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)"
      ],
      "metadata": {
        "id": "OWUWRlyX--iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: create and evaluate a BaggingClassifier with different parameters!\n",
        "# bagging_model = ...\n",
        "################################################################################\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "p\n",
        "bagging_model = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=8),\n",
        "                                   n_estimators=50,\n",
        "                                   max_features=int(np.sqrt(X.shape[1])))\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "RJYDVFhWSSNW",
        "outputId": "235745b2-33b9-4fa5-d893-c1d83555b2b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy (Decision Tree): 0.810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: create and evaluate a BaggingClassifier with a different baseline model!\n",
        "# bagging_model = ...\n",
        "################################################################################\n",
        "\n",
        "accuracy = evaluate_classifier(bagging_model, runs=10)\n",
        "print(f\"Bagging Classifier Accuracy (other baseline): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "kR5_wmVjHsRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Boosting**\n",
        "\n",
        "One of the noticeable characteristics of the Bagging algorithm is that each model was trained in a completely independent way based on a randomly selected sample. The advantage of this approach is of course that the creation of each model can be done in parallel, but a major disadvantage is that a relatively large number of models are required to achieve an aggregate prediction accuracy significantly better than that of the individual models.\n",
        "\n",
        "The latter problem is addressed by the **Boosting** family of algorithms. These methods sequentially add new models to the ensemble of models in such a way that each added model \"learns\" in some way from the weaknesses of the existing models.\n",
        "\n",
        "One of the earliest Boosting algorithms is Adaptive Boosting (**AdaBoost** for short), which means that for each model to be trained, more weight is given to the training examples for which the previous models gave incorrect predictions, so that the newly created model will perform better on those examples.\n",
        "\n",
        "The AdaBoost method consists of the following steps:\n",
        "1. As a starting step, define a uniform weight value for each record in the dataset.\n",
        "2. Train a model with the current weights.\n",
        "3. Increase the weights of the samples for which the most recent model gave an incorrect prediction, and then normalize the adjusted weight vector.\n",
        "4. Repeat step 2 until a pre-specified number of models is obtained.\n",
        "5. The prediction of the ensemble of models on new samples is obtained as the weighted aggregate of the output of each model. In the aggregation process, the weight of each model's output increases with its better predictive performance on the training set.\n",
        "\n",
        "> The AdaBoost algorithm (and Boosting methods in general) iteratively corrects errors in the existing model ensemble at each step, speeding up the convergence of the procedure. For this reason, typically fewer models are needed to achieve similarly good predictive performance when using this family of methods compared to the Bagging algorithm.\n",
        "\n",
        "The schematic flowchart of the AdaBoost algorithm for 3 models is as follows:\n",
        "<!-- ![adaboost_algorithm_simplified](https://share.mit.bme.hu/index.php/s/bW9FgHTr3xJpQdm/download/boosting_algorithm_simplified.png) -->\n",
        "\n",
        "<img src=\"https://share.mit.bme.hu/index.php/s/bW9FgHTr3xJpQdm/download/boosting_algorithm_simplified.png\" alt=\"drawing\" width=\"700\"/>\n",
        "\n",
        "As can be seen in the figure above, the learning of each model depends on the error of the previous model, so the learning of each model cannot be performed in parallel, only sequentially.\n",
        "\n",
        "There are several implementations of the AdaBoost algorithm, we will use the one included in the `sklearn` package. In practical terms, the instantiation and use of the AdaBoost classifier is similar to the bagging method implemented above:"
      ],
      "metadata": {
        "id": "4iVJ9QCIRw96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "boosted_model_dt = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                      n_estimators=20,\n",
        "                                      algorithm=\"SAMME\")\n",
        "\n",
        "accuracy = evaluate_classifier(boosted_model_dt, runs=10)\n",
        "print(f\"Boosted Classifier Accuracy (Decision Tree): {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "Pqdk1oDEpyqL",
        "outputId": "47257c63-bcd4-463d-a711-94a20291c6b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosted Classifier Accuracy (Decision Tree): 0.839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also worth comparing how AdaBoost performs against the Bagging algorithm for model ensembles of different sizes. To do this, run the training and evaluation for different model sizes and plot the results using the code block below:"
      ],
      "metadata": {
        "id": "-uieM-b91JIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "accuracy_values_bagging = []\n",
        "accuracy_values_boosting = []\n",
        "\n",
        "for n_estimators in range(1, 30, 1):\n",
        "    bagging_model_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                         n_estimators=n_estimators,\n",
        "                                         max_features=int(np.sqrt(X.shape[1])))\n",
        "    accuracy_values_bagging.append(evaluate_classifier(bagging_model_dt, runs=10))\n",
        "\n",
        "    boosted_model_dt = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3),\n",
        "                                          n_estimators=n_estimators,\n",
        "                                          algorithm=\"SAMME\")\n",
        "    accuracy_values_boosting.append(evaluate_classifier(boosted_model_dt, runs=10))\n",
        "\n",
        "plt.plot(range(1, 30, 1), accuracy_values_bagging, label=\"Bagging Classifier (Decision Tree)\")\n",
        "plt.plot(range(1, 30, 1), accuracy_values_boosting, label=\"Boosted Classifier (Decision Tree)\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim((0.5, 1.))\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "VfyXO3zisYr9",
        "outputId": "6586cf17-df48-4a86-f492-249b149b0190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Which method do you think performed better with the same base model and ensemble size?**\n",
        "\n",
        "Boosting\n",
        "\n",
        "***\n",
        "\n",
        "**Based on the results, to what extent does the predictive performance of the two methods increase with the number of models? Do you see any difference between the methods in this regard?**\n",
        "\n",
        "In a wary similar way nearly linear after a while\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "AHNZAfP91oWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it is worth mentioning that there are several boosting algorithms that are more modern than AdaBoost, of which gradient-based methods are perhaps the best known. Examples include the [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) implemented in `sklearn` and the [Extreme Gradient Boosting](https://xgboost.readthedocs.io/en/stable/), which is available as a separate package."
      ],
      "metadata": {
        "id": "DDkUX0Fs2bw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Stacking**\n",
        "\n",
        "So far, we have assumed that all members of the model ensemble are from the same model family (e.g. decision tree, SVM, etc...). However, this is not necessarily the case, since a model ensemble can consist of any type of classifiers (or regressors) as long as they offer a solution to the same classification (or regression) problem. The general case where a model ensemble is composed of estimators from different model families is called **Stacking**.\n",
        "\n",
        "The implementation is simpler than before: using `sklearn`, we initialize some individual classifiers, and then a model ensemble ([`VotingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)) from them. The resulting estimator is trained and evaluated as a single model.\n",
        "\n",
        "This can be done by running the following code block on the previously generated artificial dataset:"
      ],
      "metadata": {
        "id": "YFq0XowKSI2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "# Create a VotingClassifier with multiple base estimators\n",
        "stacking_model = VotingClassifier(estimators=[\n",
        "    ('decision_tree', DecisionTreeClassifier(max_depth=3)),\n",
        "    ('svm', SVC(kernel='rbf')),\n",
        "    ('naive_bayes', GaussianNB()),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
        "    ('gaussian_process', GaussianProcessClassifier())\n",
        "], voting='hard')\n",
        "\n",
        "accuracy = evaluate_classifier(stacking_model, runs=10)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "D_MxkqgEwp1u",
        "outputId": "3900d305-6da9-4767-ffb7-73beb8f1d0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the implementation, a cluster of 5 models was created, each of which solves the classification problem in a drastically different way. This is described in more detail in the documentation of the classes implementing each model:\n",
        "\n",
        "1. [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "2. [Support-Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        "3. [Naive Bayes](https://scikit-learn.org/dev//modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
        "4. [K-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "5. [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)\n",
        "\n",
        "> It is worth noting that the classifier created using the Stacking procedure, which includes 5 different models, has significantly better performance than the previous ensembles of models with instances of a single model type. This implies that when solving the classification problem defined by the artificially generated dataset, there is a significant advantage in the diversity of models, or in other words: in this case, the predictions of models representing different approaches complement each other well. A third (and perhaps most common) formulation of this phenomenon is that individual models in a model ensemble typically make errors on different inputs, so that the simple majority decision they make is more reliable than the prediction of each model. It is important to note, however, that this is not necessarily true for all classification (or indeed regression) problems, so individual testing of different approaches is still important. For example, it may be that a number of models perform particularly poorly on the dataset, so that the performance of the ensemble of models may be worse than the performance of some individual models."
      ],
      "metadata": {
        "id": "847fcmMd39I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Real dataset**\n",
        "\n",
        "In the following section, we will test the methods we have learned so far on a real dataset in a format similar to the artificially generated dataset we have used so far.\n",
        "\n",
        "To do this, we first reset the Colab environment and then load the dataset to be used:"
      ],
      "metadata": {
        "id": "rAfAzpPb4Ud9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "spambase_df = pd.read_csv(\"https://share.mit.bme.hu/index.php/s/wgc46gCHRb7bPdF/download/spambase.csv\")\n",
        "\n",
        "spambase_df"
      ],
      "metadata": {
        "id": "RE2zqLzZLAr2",
        "outputId": "f90a8143-20fd-43fb-d06e-678f89cf04f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
              "0               0.00               0.64           0.64           0.0   \n",
              "1               0.06               0.00           0.71           0.0   \n",
              "2               0.00               0.00           0.00           0.0   \n",
              "3               0.00               0.00           0.00           0.0   \n",
              "4               0.00               0.00           0.00           0.0   \n",
              "...              ...                ...            ...           ...   \n",
              "3676            0.00               0.00           1.19           0.0   \n",
              "3677            0.31               0.00           0.62           0.0   \n",
              "3678            0.00               0.00           0.00           0.0   \n",
              "3679            0.30               0.00           0.30           0.0   \n",
              "3680            0.00               0.00           0.65           0.0   \n",
              "\n",
              "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
              "0              0.32            0.00              0.00                0.00   \n",
              "1              1.23            0.19              0.19                0.12   \n",
              "2              0.63            0.00              0.31                0.63   \n",
              "3              0.63            0.00              0.31                0.63   \n",
              "4              1.85            0.00              0.00                1.85   \n",
              "...             ...             ...               ...                 ...   \n",
              "3676           0.00            0.00              0.00                0.00   \n",
              "3677           0.00            0.31              0.00                0.00   \n",
              "3678           0.00            0.00              0.00                0.00   \n",
              "3679           0.00            0.00              0.00                0.00   \n",
              "3680           0.00            0.00              0.00                0.00   \n",
              "\n",
              "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
              "0                0.00            0.00  ...        0.000        0.000   \n",
              "1                0.64            0.25  ...        0.010        0.143   \n",
              "2                0.31            0.63  ...        0.000        0.137   \n",
              "3                0.31            0.63  ...        0.000        0.135   \n",
              "4                0.00            0.00  ...        0.000        0.223   \n",
              "...               ...             ...  ...          ...          ...   \n",
              "3676             0.00            0.00  ...        0.000        0.000   \n",
              "3677             0.00            0.00  ...        0.000        0.232   \n",
              "3678             0.00            0.00  ...        0.000        0.000   \n",
              "3679             0.00            0.00  ...        0.102        0.718   \n",
              "3680             0.00            0.00  ...        0.000        0.000   \n",
              "\n",
              "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
              "0             0.0        0.778        0.000         0.00   \n",
              "1             0.0        0.276        0.184         0.01   \n",
              "2             0.0        0.137        0.000         0.00   \n",
              "3             0.0        0.135        0.000         0.00   \n",
              "4             0.0        0.000        0.000         0.00   \n",
              "...           ...          ...          ...          ...   \n",
              "3676          0.0        0.000        0.000         0.00   \n",
              "3677          0.0        0.000        0.000         0.00   \n",
              "3678          0.0        0.353        0.000         0.00   \n",
              "3679          0.0        0.000        0.000         0.00   \n",
              "3680          0.0        0.125        0.000         0.00   \n",
              "\n",
              "      capital_run_length_average  capital_run_length_longest  \\\n",
              "0                          3.756                          61   \n",
              "1                          9.821                         485   \n",
              "2                          3.537                          40   \n",
              "3                          3.537                          40   \n",
              "4                          3.000                          15   \n",
              "...                          ...                         ...   \n",
              "3676                       1.000                           1   \n",
              "3677                       1.142                           3   \n",
              "3678                       1.555                           4   \n",
              "3679                       1.404                           6   \n",
              "3680                       1.250                           5   \n",
              "\n",
              "      capital_run_length_total  Class  \n",
              "0                          278      1  \n",
              "1                         2259      1  \n",
              "2                          191      1  \n",
              "3                          191      1  \n",
              "4                           54      1  \n",
              "...                        ...    ...  \n",
              "3676                        24      0  \n",
              "3677                        88      0  \n",
              "3678                        14      0  \n",
              "3679                       118      0  \n",
              "3680                        40      0  \n",
              "\n",
              "[3681 rows x 58 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb390d12-1541-4b19-a471-cbbc31e1ee28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>...</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.01</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3676</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3677</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3681 rows Ã 58 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb390d12-1541-4b19-a471-cbbc31e1ee28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb390d12-1541-4b19-a471-cbbc31e1ee28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb390d12-1541-4b19-a471-cbbc31e1ee28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c162bb9a-d589-4e33-ac3f-e8a7630cc952\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c162bb9a-d589-4e33-ac3f-e8a7630cc952')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c162bb9a-d589-4e33-ac3f-e8a7630cc952 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_272196ad-4f7d-4782-9449-9321832e1d25\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('spambase_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_272196ad-4f7d-4782-9449-9321832e1d25 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('spambase_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "spambase_df"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above database contains a total of 57 continuous-valued variables and one binary target variable. The input variables are descriptive statistics extracted from emails, giving the relative frequency of occurrence of each word and character (in percentage terms, variables starting with `word_freq_*` and `char_freq_*`), the average length of all uppercase character sequences, the length of the longest such sequence, and the total number of uppercase characters. The target variable is `Class`, shown in the last column of the dataset, which indicates whether the original email is considered spam (`Class=1`) or not (`Class=0`).\n",
        "\n",
        "Then, in preparation for training, let us split the dataset into training and test subsets:"
      ],
      "metadata": {
        "id": "rV-b3OU0JNue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = spambase_df.drop(\"Class\", axis=1).values\n",
        "y = spambase_df[\"Class\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "JKHdr9ZRRKHQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Task submission**\n",
        "\n",
        "The task is to create a spam classifier using the model ensembles described above and the `spambase` dataset.\n",
        "\n",
        "**Do this by modifying the skeleton of the `spam_predictor.py` file shown below!**\n",
        "\n",
        "Modify the `predict` function in the skeleton to create a model ensemble of any type, train it on the training data (`X_train` and `y_train`) given as parameters, and return with predictions on the test set (`X_test`) given as a parameter.\n",
        "\n",
        "> Hint: The `sklearn` library is installed on the Moodle evaluation server, so you should use the [model ensemble methods](https://scikit-learn.org/stable/api/sklearn.ensemble.html) and [classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) provided there."
      ],
      "metadata": {
        "id": "ctuFvXq84XoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def predict(X_train, y_train, X_test):\n",
        "\n",
        "    ensemble_model = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=3),\n",
        "        n_estimators=50,\n",
        "        learning_rate=1.0,\n",
        "        random_state=42,\n",
        "        algorithm=\"SAMME\"\n",
        "    )\n",
        "\n",
        "\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "    return ensemble_model.predict(X_test)"
      ],
      "metadata": {
        "id": "vam0-xBNTiL1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then check the accuracy of the `predict` function defined above on the test set:"
      ],
      "metadata": {
        "id": "r92bdywISLQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = predict(X_train, y_train, X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Ensemble Classifier Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "ENZbIRkESH5x",
        "outputId": "7ddadb3d-7b17-401d-e0f0-6bb284a4be3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Classifier Accuracy: 0.948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Moodle assignment for the lab is considered complete if a separate test set in the `spambase` database achieves at least `93%` accuracy. The test set used in the Moodle system **is not identical to the test set seen here**, but contains the same variables.\n",
        "\n",
        "> **Important:** When submitting the assignment, please make sure that the file name is `spam_predictor.py` and that the `predict` function is defined in the same way as shown above."
      ],
      "metadata": {
        "id": "GAq31x5tSpig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submission:\n",
        "\n",
        "https://edu.vik.bme.hu/mod/quiz/view.php?id=152403"
      ],
      "metadata": {
        "id": "8dCJXEC8WTWS"
      }
    }
  ]
}